ncnn
------------------------------------------
ncnn is a high-performance neural network inference computing framework
optimized for mobile platforms. ncnn is deeply considerate about deployment
and uses on mobile phones from the beginning of design. ncnn does not have
third party dependencies. it is cross-platform, and runs faster than all known
open source frameworks on mobile phone cpu. Developers can easily deploy deep
learning algorithm models to the mobile platform by using efficient ncnn
implementation, create intelligent APPs, and bring the artificial intelligence
to your fingertips. ncnn is currently being used in many Tencent applications,
such as QQ, Qzone, WeChat, Pitu and so on.

Runtime requirements:
  cygwin-3.6.6-1
  libgcc1-13.4.0-1
  libgomp1-13.4.0-1
  libncnn-devel-20260113-1bl1
  libncnn1-20260113-1bl1
  libprotobuf32-21.12-1
  libstdc++6-13.4.0-1
  pkg-config-2.5.1-1

Build requirements:
(besides corresponding -devel packages)
  binutils-2.45.1-1
  cmake-4.2.1-1
  cygport-0.37.6-1
  gcc-core-13.4.0-1
  gcc-g++-13.4.0-1
  libprotobuf-devel-21.12-1
  ninja-1.13.2-1

Canonical website:
  https://github.com/Tencent/ncnn

Canonical download:
  https://github.com/Tencent/ncnn/archive/refs/tags/20260113.tar.gz

-------------------------------------------

Build instructions:
  1. unpack ncnn-20260113-X-src.tar.xz
  2. if you use setup to install this src package,
     it will be unpacked under /usr/src automatically
        % cd /usr/src
        % cygport ./ncnn-20260113-X.cygport all

This will create:
  /usr/src/ncnn-20260113-X-src.tar.xz
  /usr/src/ncnn-20260113-X.tar.xz
  /usr/src/libncnn1-20260113-X.tar.xz
  /usr/src/libncnn-devel-20260113-X.tar.xz

-------------------------------------------

Files included in the binary package:

(ncnn)
  /usr/bin/caffe2ncnn.exe
  /usr/bin/darknet2ncnn.exe
  /usr/bin/mxnet2ncnn.exe
  /usr/bin/ncnn2int8.exe
  /usr/bin/ncnn2mem.exe
  /usr/bin/ncnn2table.exe
  /usr/bin/ncnnmerge.exe
  /usr/bin/ncnnoptimize.exe
  /usr/share/doc/Cygwin/ncnn.README
  /usr/share/doc/ncnn/LICENSE.txt
  /usr/share/doc/ncnn/README.md
  /usr/share/doc/ncnn/docs/Home.md
  /usr/share/doc/ncnn/docs/application-with-ncnn-inside.md
  /usr/share/doc/ncnn/docs/benchmark/the-benchmark-of-caffe-android-lib,-mini-caffe,-and-ncnn.md
  /usr/share/doc/ncnn/docs/benchmark/vulkan-conformance-test.md
  /usr/share/doc/ncnn/docs/developer-guide/aarch64-mix-assembly-and-intrinsic.md
  /usr/share/doc/ncnn/docs/developer-guide/add-custom-layer.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/arm-a53-a55-dual-issue.md
  /usr/share/doc/ncnn/docs/developer-guide/armv7-mix-assembly-and-intrinsic.md
  /usr/share/doc/ncnn/docs/developer-guide/binaryop-broadcasting.md
  /usr/share/doc/ncnn/docs/developer-guide/build-ncnn-on-windows-xp.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/custom-allocator.md
  /usr/share/doc/ncnn/docs/developer-guide/element-packing.md
  /usr/share/doc/ncnn/docs/developer-guide/expression.md
  /usr/share/doc/ncnn/docs/developer-guide/glsl-extension.md
  /usr/share/doc/ncnn/docs/developer-guide/glsl-extension.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/how-to-be-a-contributor.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/how-to-implement-custom-layer-step-by-step.md
  /usr/share/doc/ncnn/docs/developer-guide/how-to-write-a-neon-optimized-op-kernel.md
  /usr/share/doc/ncnn/docs/developer-guide/how-to-write-a-sse-optimized-op-kernel.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/kvcache.md
  /usr/share/doc/ncnn/docs/developer-guide/layer-feat-mask.md
  /usr/share/doc/ncnn/docs/developer-guide/layer-support-behavior.md
  /usr/share/doc/ncnn/docs/developer-guide/low-level-operation-api.md
  /usr/share/doc/ncnn/docs/developer-guide/ncnn-tips-and-tricks.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/new-model-load-api.md
  /usr/share/doc/ncnn/docs/developer-guide/new-param-load-api.md
  /usr/share/doc/ncnn/docs/developer-guide/operation-param-weight-table.md
  /usr/share/doc/ncnn/docs/developer-guide/operators.md
  /usr/share/doc/ncnn/docs/developer-guide/param-and-model-file-structure.md
  /usr/share/doc/ncnn/docs/developer-guide/preload-practice.zh.md
  /usr/share/doc/ncnn/docs/developer-guide/tensorflow-op-combination.md
  /usr/share/doc/ncnn/docs/developer-guide/vulkan-driver-loader.md
  /usr/share/doc/ncnn/docs/faq.en.md
  /usr/share/doc/ncnn/docs/faq.md
  /usr/share/doc/ncnn/docs/how-to-build/build-mlir2ncnn.md
  /usr/share/doc/ncnn/docs/how-to-build/how-to-build.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/FAQ-ncnn-produce-wrong-result.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/FAQ-ncnn-protobuf-problem.zh.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/FAQ-ncnn-throw-error.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/FAQ-ncnn-vulkan.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/build-minimal-library.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/efficient-roi-resize-rotate.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/ncnn-load-model.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/openmp-best-practice.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/openmp-best-practice.zh.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/quantized-int8-inference.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/use-ncnn-with-alexnet.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/use-ncnn-with-alexnet.zh.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/use-ncnn-with-opencv.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/use-ncnn-with-own-project.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/use-ncnn-with-pytorch-or-onnx.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/use-ncnnoptimize-to-optimize-model.md
  /usr/share/doc/ncnn/docs/how-to-use-and-FAQ/vulkan-notes.md

(libncnn1)
  /usr/bin/cygncnn-1.dll

(libncnn-devel)
  /usr/include/ncnn/allocator.h
  /usr/include/ncnn/benchmark.h
  /usr/include/ncnn/blob.h
  /usr/include/ncnn/c_api.h
  /usr/include/ncnn/command.h
  /usr/include/ncnn/cpu.h
  /usr/include/ncnn/datareader.h
  /usr/include/ncnn/expression.h
  /usr/include/ncnn/gpu.h
  /usr/include/ncnn/layer.h
  /usr/include/ncnn/layer_shader_type.h
  /usr/include/ncnn/layer_shader_type_enum.h
  /usr/include/ncnn/layer_type.h
  /usr/include/ncnn/layer_type_enum.h
  /usr/include/ncnn/mat.h
  /usr/include/ncnn/modelbin.h
  /usr/include/ncnn/ncnn_export.h
  /usr/include/ncnn/net.h
  /usr/include/ncnn/option.h
  /usr/include/ncnn/paramdict.h
  /usr/include/ncnn/pipeline.h
  /usr/include/ncnn/pipelinecache.h
  /usr/include/ncnn/platform.h
  /usr/include/ncnn/simplemath.h
  /usr/include/ncnn/simpleocv.h
  /usr/include/ncnn/simpleomp.h
  /usr/include/ncnn/simplestl.h
  /usr/include/ncnn/simplevk.h
  /usr/include/ncnn/vulkan_header_fix.h
  /usr/lib/cmake/ncnn/ncnn-release.cmake
  /usr/lib/cmake/ncnn/ncnn.cmake
  /usr/lib/cmake/ncnn/ncnnConfig.cmake
  /usr/lib/cmake/ncnn/ncnnConfigVersion.cmake
  /usr/lib/libncnn.dll.a
  /usr/lib/pkgconfig/ncnn.pc

------------------

Port Notes:

----- version 20260113-1bl1 -----
Version bump.

----- version 20241226-1bl1 -----
Version bump.

----- version 20240820-1bl1 -----
Version bump.

----- version 20240410-1bl1 -----
Version bump.

----- version 20240102-1bl1 -----
Version bump.

----- version 20231027-1bl1 -----
Version bump.

----- version 20230816-1bl1 -----
Version bump.

----- version 20221128-1bl1 -----
Version bump.

----- version 20220701-1bl1 -----
Version bump.

----- version 20210720-1bl1 -----
Version bump.

----- version 20210525-1bl1 -----
Version bump.

----- version 20210322-1bl1 -----
Version bump.

----- version 20200916-1bl1 -----
Version bump.

----- version 20200909-1bl1 -----
Version bump.

----- version 20200413-1bl1 -----
Version bump.

----- version 20190908-1bl1 -----
Initial release by fd0 <https://github.com/fd00/>
